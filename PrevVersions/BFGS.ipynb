{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MINIMISE \n",
    "$\\\\f_1(x) = 10x_1 + 2x_2\\\\$\n",
    "$f_2(x) = 3x_1 ^ 2 + 2x_2 ^ 2$ \n",
    "\n",
    "SUBJECT TO\n",
    "$\\\\ x_1 \\geq 2\\\\$\n",
    "$ x_2 \\geq 3 \\\\$\n",
    "$ x_1 + x_2 \\leq 12$\n",
    "\n",
    "Both f_1 and f_2 are continously differentiable \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aim of problem\n",
    "min f(x) sub to contraints\n",
    "Quasi-Newton methods\n",
    "B_n approx = nabla ^2 f(x_n)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO :UPDATE CALC AS INDICATED IN ORIGINAL PAPER\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "def search_dir(x, grads, B,n_args, printRes = False):\n",
    "    \"\"\"\n",
    "    Calcualtes the search direction of a function F from its gradient,\n",
    "    and hessian approximations.\n",
    "\n",
    "    INPUT: x - the current point\n",
    "           grads - a list of gradient functions\n",
    "           B - a list of hessian approximations TODO:check\n",
    "           n_args - the number of arguments of the function\n",
    "           printRes - a boolean to print the result\n",
    "    OUTPUT: dOpt - the optimal search direction\n",
    "            thetaOpt - the optimal value of the objective function\n",
    "    \"\"\"\n",
    "    \n",
    "    #Calculate d(x) = arg min d(x) {max_i {grad_i(x)^T d + 0.5 d^T B_i d}}\n",
    "    def obj(d,B,grads, x):\n",
    "        #Function to maximise\n",
    "        return np.max([(grad(x)).T @ d + 0.5 * d.T @ B_j @ d for grad, B_j in zip(grads, B)])\n",
    "    \n",
    "    #Initial guess for d\n",
    "    d0 = np.zeros(n_args)\n",
    "    result = minimize(obj, d0, args=(B, grads,x))\n",
    "    \n",
    "    dOpt = result.x\n",
    "\n",
    "    #Calculate theta(x) = max_i {grad_i(x)^T d + 0.5 d^T B_i d}\n",
    "    thetaOpt = obj(dOpt, B, grads, x)\n",
    "\n",
    "    return dOpt, thetaOpt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Delta(grads,x,d):\n",
    "    return np.max([grad(x).T @ d for grad in grads])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: add tolerance?\n",
    "def Armijo(F,grads, x, alpha, d, c1):\n",
    "    for f in F:\n",
    "        if f(x + alpha * d) > f(x) + c1 * alpha * Delta(grads, x, d):\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: add tolerance ?\n",
    "def Curvature(F, grads, x, alpha, d, c2):\n",
    "    if Delta(grads,x + alpha * d,d) >= c2 * Delta(grads, x,d):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WLinesearch(F,grads, x,d,c1 = 0.1, c2 = 0.9):\n",
    "    \"\"\"\"\n",
    "    Linesearch\n",
    "    Inputs: F - a list of functions\n",
    "            grads - a list of gradient functions\n",
    "            x - the current point\n",
    "            d - the search direction\n",
    "            c1 - the Armijo condition parameter\n",
    "            c2 - the curvature condition parameter\n",
    "    Outputs: alpha - the step size\n",
    "    \"\"\"\n",
    "    alpha = 1\n",
    "    alpha_bar = 0\n",
    "    alpha_hat = 0\n",
    "    armijo_test = Armijo(F, grads, x, alpha, d, c1)\n",
    "    curvature_test = Curvature(F, grads, x, alpha, d, c2)\n",
    "\n",
    "    while not armijo_test or not curvature_test:\n",
    "        if not armijo_test:\n",
    "            alpha_bar = alpha\n",
    "            alpha = (alpha_bar + alpha_hat) / 2\n",
    "        elif not curvature_test:\n",
    "            alpha_bar = alpha\n",
    "\n",
    "            if alpha_hat == 0:\n",
    "                alpha = 2 * alpha_bar\n",
    "            else:\n",
    "                alpha = (alpha_bar + alpha_hat) / 2\n",
    "        \n",
    "        armijo_test = Armijo(F, grads, x, alpha, d, c1)\n",
    "        curvature_test = Curvature(F, grads, x, alpha, d, c2)\n",
    "\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import array\n",
    "\n",
    "def BUpdate(F, grads,B0, x0, x1):\n",
    "    #Prep\n",
    "    grads_x0 = np.array([grad(x0) for grad in grads])\n",
    "    grads_x1 = np.array([grad(x1) for grad in grads])\n",
    "\n",
    "    y = grads_x1 - grads_x0\n",
    "    s = x1 - x0\n",
    "    sy = s.T @ y\n",
    "    #rho\n",
    "    if sy> 0:\n",
    "        rho_over = 1/sy\n",
    "    else:\n",
    "        rho_over = 1/ (Delta(grads,x1,s) - grads_x0.T @ s)\n",
    "\n",
    "    \n",
    "    denom = 1/((rho_over - s.T @ y)**2 + rho_over * s.T @ B0 @ s)\n",
    "\n",
    "    #Update B\n",
    "    t1 = rho_over * B0 @ s @ s.T @ B0\n",
    "    t2 = (s.T @ B0 @ s) * (y @ y.T)\n",
    "    t3 = (rho_over - s.T@ y) * (y @ s.T @ B0 + B0 @ s @ y.T)\n",
    "          \n",
    "    B1 = B0  + denom * (-t1 + t2 + t3)\n",
    "    return B1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BFGS(F,grads, x, B, c1, c2, iter = 100, tol = 1e-6):\n",
    "\n",
    "    n_args = len(x)\n",
    "    x0 = x\n",
    "    Bn = B\n",
    "    for i in range(iter):\n",
    "\n",
    "        #STEP 1: Search direction\n",
    "        d,theta = search_dir(x0, grads, Bn, n_args)\n",
    "\n",
    "        #STEP 2: Stopping criterion\n",
    "        if abs(theta) < tol:\n",
    "            return x\n",
    "        \n",
    "        #STEP 3: Line search\n",
    "        alpha = WLinesearch(F, grads, x, d, c1, c2)\n",
    "        xi = x + alpha * d\n",
    "\n",
    "        sk = xi - x\n",
    "\n",
    "        Bn = BUpdate(F, grads, Bn, x0, xi)\n",
    "    \n",
    "    print(\"BFGS did not converge\")\n",
    "    return xi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "YLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
